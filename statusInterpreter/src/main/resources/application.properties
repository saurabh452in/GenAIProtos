server.port=8081
spring.application.name=statusInterpreter
spring.ai.model.chat=ollama
spring.ai.ollama.chat.options.model=llama3.2:3b
spring.ai.ollama.chat.options.temperature=0.5
spring.ai.chat.client.observations.log-prompt=true
spring.ai.chat.observations.log-prompt=true
spring.ai.chat.observations.log-completion=true